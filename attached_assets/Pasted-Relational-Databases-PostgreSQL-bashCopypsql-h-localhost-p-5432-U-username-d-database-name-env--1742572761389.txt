Relational Databases
PostgreSQL
bashCopypsql -h localhost -p 5432 -U username -d database_name
.env variables:
CopyPOSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=username
POSTGRES_PASSWORD=your_password
POSTGRES_DB=database_name
MySQL/MariaDB
bashCopymysql -h localhost -P 3306 -u username -p database_name
.env variables:
CopyMYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_USER=username
MYSQL_PASSWORD=your_password
MYSQL_DATABASE=database_name
SQL Server
bashCopysqlcmd -S localhost -U username -P password -d database_name
.env variables:
CopyMSSQL_SERVER=localhost
MSSQL_USER=username
MSSQL_PASSWORD=your_password
MSSQL_DATABASE=database_name
Oracle Database
bashCopysqlplus username/password@//localhost:1521/service_name
.env variables:
CopyORACLE_HOST=localhost
ORACLE_PORT=1521
ORACLE_SERVICE=service_name
ORACLE_USER=username
ORACLE_PASSWORD=your_password
SQLite
bashCopysqlite3 /path/to/database.db
.env variables:
CopySQLITE_DB_PATH=/path/to/database.db
Amazon Redshift
bashCopypsql -h redshift-cluster-name.region.redshift.amazonaws.com -p 5439 -U username -d database_name
.env variables:
CopyREDSHIFT_HOST=redshift-cluster-name.region.redshift.amazonaws.com
REDSHIFT_PORT=5439
REDSHIFT_USER=username
REDSHIFT_PASSWORD=your_password
REDSHIFT_DATABASE=database_name
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=your_region
Google Cloud SQL
bashCopy# First authenticate (interactive login)
gcloud auth login
# Then connect
gcloud sql connect instance-name --user=username
.env variables:
CopyGOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
GCP_PROJECT_ID=your_project_id
GCP_SQL_INSTANCE=instance_name
GCP_SQL_USER=username
GCP_SQL_PASSWORD=your_password
GCP_SQL_DATABASE=database_name
IBM Db2
bashCopydb2 connect to database_name user username using password
.env variables:
CopyDB2_DATABASE=database_name
DB2_HOSTNAME=hostname
DB2_PORT=50000
DB2_USERNAME=username
DB2_PASSWORD=your_password
NoSQL Databases
MongoDB
bashCopymongo "mongodb://username:password@hostname:port/database_name"
.env variables:
CopyMONGO_URI=mongodb://username:password@hostname:port/database_name
MONGO_HOST=hostname
MONGO_PORT=port
MONGO_USER=username
MONGO_PASSWORD=your_password
MONGO_DATABASE=database_name
Firebase Firestore
bashCopy# First authenticate (interactive Google login)
firebase login
# Then access Firestore
firebase firestore:get --project project_id collections/documents
.env variables:
CopyFIREBASE_API_KEY=your_api_key
FIREBASE_AUTH_DOMAIN=your_project_id.firebaseapp.com
FIREBASE_PROJECT_ID=your_project_id
FIREBASE_STORAGE_BUCKET=your_project_id.appspot.com
FIREBASE_MESSAGING_SENDER_ID=your_messaging_sender_id
FIREBASE_APP_ID=your_app_id
FIREBASE_MEASUREMENT_ID=your_measurement_id
FIREBASE_DATABASE_URL=https://your_project_id.firebaseio.com
FIREBASE_SERVICE_ACCOUNT_KEY_PATH=/path/to/serviceAccountKey.json
DynamoDB (AWS CLI)
bashCopy# First configure AWS credentials (interactive)
aws configure
# Then access DynamoDB
aws dynamodb list-tables --endpoint-url http://localhost:8000
.env variables:
CopyAWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=your_region
AWS_DYNAMODB_ENDPOINT=http://localhost:8000
Azure Cosmos DB
bashCopy# First login to Azure (interactive)
az login
# Then query Cosmos DB
az cosmosdb sql query --account-name account_name --database-name database_name --container-name container_name --query-text "SELECT * FROM c"
.env variables:
CopyAZURE_COSMOS_ENDPOINT=https://your-account.documents.azure.com:443/
AZURE_COSMOS_KEY=your_primary_key
AZURE_COSMOS_DATABASE=database_name
AZURE_COSMOS_CONTAINER=container_name
Supabase
bashCopy# Interactive login first
supabase login
# Then connect to the database
psql "postgres://postgres:password@db.supabase.co:5432/postgres"
.env variables:
CopySUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
SUPABASE_DB_URL=postgres://postgres:password@db.supabase.co:5432/postgres
Heroku Postgres
bashCopy# First login (interactive)
heroku login
# Then connect
heroku pg:psql postgresql-shaped-12345 --app your-app-name
.env variables:
CopyHEROKU_API_KEY=your_api_key
HEROKU_APP_NAME=your_app_name
DATABASE_URL=postgres://username:password@hostname:port/database_name
Snowflake
bashCopy# Interactive login
snowsql -a account_identifier -u username
# After login prompt for password
.env variables:
CopySNOWFLAKE_ACCOUNT=your_account_identifier
SNOWFLAKE_USER=username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ROLE=your_role
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
Google BigQuery
bashCopy# First authenticate (interactive)
gcloud auth login
# Then query
bq query --use_legacy_sql=false 'SELECT * FROM dataset.table LIMIT 10'
.env variables:
CopyGOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
GCP_PROJECT_ID=your_project_id
BIGQUERY_DATASET=your_dataset